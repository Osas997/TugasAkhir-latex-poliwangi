%==================================================================
% Ini adalah bab 4
% Silahkan edit sesuai kebutuhan, baik menambah atau mengurangi \section, \subsection
%==================================================================

\chapter[HASIL DAN PEMBAHASAN]{\\ HASIL DAN PEMBAHASAN}

\section{Hasil}
Bagian ini memaparkan hasil penelitian yang disusun secara sistematis berdasarkan lima tahapan dalam Fountain Model sebagaimana telah diuraikan pada Bab 3. Setiap subseksi menjelaskan Hasil Analisis Kebutuhan dan temuan pada masing-masing tahap pengembangan sistem, Hasil Spesifikasi Sistem, Hasil Desain Sistem, Hasil Implementasi Sistem, Hasil Pengujian dan Integrasi

\subsection{Hasil Analisis Kebutuhan Sistem (Analysis)}
Tahap analisis kebutuhan sistem berhasil mengidentifikasi permasalahan utama yang melatarbelakangi penelitian ini, yaitu kesulitan pelaku UMKM dalam memahami hasil analisis sentimen yang umumnya disajikan dalam bentuk visualisasi data seperti grafik dan tabel. Hasil menunjukkan bahwa penyajian data secara visual saja belum cukup membantu, karena pengguna membutuhkan penjelasan yang lebih naratif dan mudah dipahami melalui interaksi berbasis bahasa natural.

Analisis lebih lanjut terhadap karakteristik pengguna mengungkapkan bahwa sebagian besar pelaku UMKM tidak memiliki latar belakang teknis di bidang statistik maupun \textit{data science}. Kondisi ini menyebabkan interpretasi terhadap visualisasi data yang kompleks menjadi tidak optimal. Temuan tersebut memperkuat kebutuhan akan sebuah sistem chatbot yang mampu menjembatani kesenjangan antara data analitik yang bersifat teknis dengan interpretasi praktis yang relevan untuk pengambilan keputusan bisnis.

Pada tahap ini juga berhasil ditetapkan batasan ruang lingkup penelitian secara jelas. Penelitian tidak mencakup proses \textit{scraping} data Instagram maupun pengembangan model analisis sentimen, karena kedua komponen tersebut dikembangkan oleh tim kolaborasi yang berbeda. Fokus penelitian diarahkan pada pengembangan sistem backend berbasis NestJS serta implementasi chatbot berbasis \textit{Retrieval-Augmented Generation (RAG)} yang memanfaatkan hasil analisis sentimen sebagai \textit{knowledge base}.

Hasil analisis kebutuhan pengguna menunjukkan bahwa sistem yang dikembangkan harus mampu menyediakan antarmuka untuk mengakses informasi sentimen, memberikan interpretasi kontekstual terhadap data numerik, serta menyajikan rekomendasi strategis yang dapat langsung diterapkan oleh pelaku UMKM. Kebutuhan-kebutuhan inilah yang kemudian menjadi landasan utama dalam perumusan spesifikasi sistem pada tahap selanjutnya.

\subsection{Hasil Spesifikasi Sistem (Requirements Specifications)}
Berdasarkan hasil analisis kebutuhan, tahap spesifikasi sistem menghasilkan dokumentasi kebutuhan fungsional dan non-fungsional yang disusun secara terstruktur. Kebutuhan fungsional sistem dirumuskan ke dalam empat komponen utama yang menjadi tulang punggung aplikasi. Komponen pertama adalah sistem autentikasi dan autorisasi berbasis \textit{JSON Web Token (JWT)} yang berperan memastikan keamanan akses pengguna. Mekanisme JWT dipilih karena bersifat \textit{stateless} dan efisien, sehingga proses verifikasi identitas dapat dilakukan tanpa menyimpan \textit{session} di sisi server.

Komponen kedua adalah modul manajemen data hasil \textit{scraping} Instagram UMKM yang menyediakan fungsionalitas \textit{create} dan \textit{delete}. Modul ini dirancang untuk memudahkan pengelolaan data mentah yang digunakan sebagai input utama dalam proses analisis sentimen. Komponen ketiga berupa \textit{API Gateway} yang berfungsi sebagai lapisan orkestrasi antara frontend dan \textit{microservices} eksternal, khususnya layanan \textit{Aspect-Based Sentiment Analysis (ABSA)} dan sistem rekomendasi konten. Penerapan API Gateway memberikan fleksibilitas dalam pengembangan serta pemeliharaan masing-masing layanan secara independen.

Komponen keempat sekaligus menjadi fokus utama penelitian ini adalah chatbot berbasis \textit{Retrieval-Augmented Generation (RAG)}. Chatbot dirancang untuk memungkinkan pengguna berinteraksi menggunakan bahasa natural guna memperoleh interpretasi yang kontekstual terhadap hasil analisis sentimen. Pendekatan RAG dipilih karena kemampuannya mengombinasikan proses \textit{retrieval} dokumen relevan dengan proses \textit{generation} jawaban menggunakan \textit{Large Language Model}, sehingga respons yang dihasilkan lebih akurat dan berbasis data faktual.

Selain kebutuhan fungsional, spesifikasi kebutuhan non-fungsional juga ditetapkan dengan mempertimbangkan aspek performa, keamanan, dan \textit{maintainability} sistem. Aspek keamanan menjadi prioritas utama dalam spesifikasi non-fungsional. Mekanisme keamanan yang diterapkan meliputi penggunaan algoritma \textit{bcrypt} untuk proses hashing password dengan \textit{salt rounds} yang memadai, validasi JWT pada setiap endpoint yang memerlukan autentikasi, sanitasi input untuk mencegah berbagai bentuk serangan injeksi, serta penggunaan protokol HTTPS untuk mengenkripsi komunikasi antara client dan server. Kombinasi mekanisme tersebut dirancang untuk melindungi data pengguna sekaligus menjaga integritas sistem secara keseluruhan.

Dari sisi \textit{maintainability}, sistem dirancang dengan menerapkan arsitektur modular yang mengikuti prinsip \textit{SOLID}, serta dokumentasi kode yang memadai. Pendekatan ini memastikan bahwa sistem dapat dipelihara dan dikembangkan lebih lanjut dengan lebih mudah oleh pengembang lain, tanpa memerlukan waktu yang lama untuk memahami struktur dan alur kode yang telah ada.

\subsection{Hasil Desain Sistem (Design)}
Tahap desain sistem menghasilkan sebuah \textit{blueprint} arsitektur yang detail, terstruktur, dan selaras dengan kebutuhan yang telah diidentifikasi pada tahap sebelumnya. Secara konseptual, arsitektur dibagi ke dalam tiga lapisan utama dengan tanggung jawab yang jelas dan terpisah, sehingga mendukung prinsip \textit{separation of concerns}, meningkatkan \textit{maintainability}, serta memudahkan \textit{scalability} sistem.

Lapisan presentasi diimplementasikan menggunakan React.js sebagai \textit{frontend framework} yang bertanggung jawab terhadap interaksi dan pengalaman pengguna. Frontend dirancang untuk menyajikan visualisasi data sentimen dalam bentuk grafik dan dashboard interaktif, sekaligus menyediakan antarmuka chatbot untuk komunikasi berbasis bahasa natural. Seluruh pemrosesan data, logika bisnis, dan pengambilan keputusan dilakukan pada sisi backend, sedangkan frontend berperan sebagai \textit{presentation layer} yang menyajikan informasi secara intuitif dan mudah dipahami oleh pengguna.

Lapisan aplikasi dibangun menggunakan NestJS dan berfungsi sebagai inti operasional sistem. Backend NestJS mengelola sejumlah modul fungsional yang saling terintegrasi, antara lain modul autentikasi yang menangani proses registrasi, login, dan validasi token autentikasi, modul manajemen data scraping yang menyediakan endpoint \textit{create} dan \textit{delete} untuk mengelola data hasil scraping Instagram, modul \textit{API Gateway} yang bertindak sebagai \textit{proxy} dalam meneruskan permintaan ke layanan external seperto \textit{Aspect-Based Sentiment Analysis (ABSA)} dan sistem rekomendasi, serta modul RAG sebagai komponen inti yang mengimplementasikan pipeline \textit{Retrieval-Augmented Generation} untuk mendukung fungsionalitas chatbot.

Lapisan data menggunakan PostgreSQL dengan ekstensi \textit{pgvector} sebagai tempat penyimpanan. Desain basis data menerapkan pendekatan hibrida yang mengombinasikan model relasional ter-normalisasi untuk data struktural dengan penyimpanan berbasis vektor untuk mendukung kebutuhan RAG. Skema database divisualisasikan melalui \textit{Entity Relationship Diagram (ERD)} yang menggambarkan relasi antar tabel secara rinci, mencakup tabel \texttt{users}, \texttt{scrape\_results}, \texttt{sentiment\_result}, \texttt{sentiment\_comments}, \texttt{recommendation\_result}, serta tabel khusus \texttt{langchain\_documents} untuk penyimpanan embedding vektor.

Salah satu keluaran utama dari tahap desain adalah perancangan pipeline RAG yang terdiri dari dua fase utama yang saling terkait namun dapat berjalan secara independen. \textit{RAG Ingestion Pipeline} dirancang untuk memproses file JSON hasil analisis sentimen menjadi dokumen-dokumen terstruktur, mengonversinya ke dalam bentuk embedding, dan menyimpannya ke dalam \textit{vector database}. Pipeline ini menerapkan prinsip \textit{idempotent}, sehingga proses ingestion dapat dijalankan berulang kali tanpa menimbulkan duplikasi data.

Di sisi lain, \textit{RAG Query Pipeline} dirancang untuk menangani proses pertanyaan pengguna hingga menghasilkan jawaban yang relevan. Pipeline ini mencakup tahap \textit{query preprocessing}, konversi pertanyaan menjadi embedding, pencarian dokumen relevan menggunakan \textit{similarity search} dengan metrik \textit{cosine similarity}, konstruksi prompt yang menggabungkan konteks hasil retrieval dengan pertanyaan pengguna, serta generasi jawaban menggunakan \textit{Large Language Model}. Seluruh tahapan dirancang secara modular untuk memudahkan pengujian, pemeliharaan, dan pengembangan lanjutan.

Selain itu, Use Case Diagram yang dihasilkan pada tahap desain memberikan gambaran menyeluruh mengenai interaksi antara pengguna dan sistem. Diagram ini mencakup use case utama seperti \textit{Register}, \textit{Login}, \textit{Landing Page}, \textit{Melihat Overall Sentiment}, \textit{Chatbot Sentimen}, \textit{Data Scraper}, serta \textit{Melihat Rekomendasi Konten}. Relasi antar use case didefinisikan menggunakan relasi \textit{include} dan \textit{extend}, yang menunjukkan ketergantungan serta alur interaksi sistem secara logis dan terstruktur.

\subsection{Hasil Implementasi Sistem (Coding)}

Tahap implementasi sistem merupakan fase penerjemahan rancangan yang telah disusun pada tahap desain ke dalam bentuk sistem yang dapat dijalankan secara nyata. Pada penelitian ini, implementasi difokuskan pada pembangunan sistem backend untuk chatbot analisis sentimen UMKM berbasis web dengan integrasi model \textit{Retrieval-Augmented Generation (RAG)}. Seluruh alur kendali sistem dijalankan melalui API backend yang dikembangkan menggunakan NestJS dan LangChain.js sebagai orkestrator pipeline RAG. Proses implementasi mengacu pada \textit{Model Fountain}, yang memungkinkan pengembangan dilakukan secara paralel dan iteratif, sehingga setiap komponen sistem dapat dibangun, diuji, dan disempurnakan secara bertahap tanpa harus menunggu penyelesaian komponen lainnya.

Ruang lingkup implementasi mencakup beberapa aspek utama yang telah dirancang pada tahap desain, meliputi penyiapan lingkungan pengembangan, pembangunan basis data, pengembangan arsitektur backend, serta integrasi pipeline \textit{Retrieval-Augmented Generation (RAG)}. Seluruh aspek tersebut dirancang agar saling terhubung dan membentuk sebuah sistem chatbot analisis sentimen UMKM yang utuh dan terintegrasi. Mengingat fokus penelitian ini berada pada pengembangan backend API, pembahasan mengenai sisi frontend yang dikembangkan oleh rekan kolaborasi tidak dijelaskan secara rinci dalam laporan ini.

\begin{enumerate}
  \item \textbf{Hasil Implementasi Lingkungan Pengembangan dan Konfigurasi}

        \hspace*{1.25cm}Lingkungan pengembangan sistem backend berhasil disiapkan dengan menggunakan Node.js versi LTS sebagai \textit{runtime environment} guna menjamin stabilitas serta dukungan jangka panjang. Framework NestJS dipilih untuk membangun aplikasi backend karena menyediakan struktur modular yang rapi dan terstandarisasi, sehingga sesuai untuk pengembangan aplikasi berskala menengah hingga besar. Proses pengembangan dilakukan dengan memanfaatkan NestJS CLI yang membantu pembuatan struktur proyek secara cepat, konsisten, dan terorganisir sejak tahap awal.

        \hspace*{1.25cm}Visual Studio Code digunakan sebagai \textit{Integrated Development Environment (IDE)} utama. Pemilihan IDE ini didukung oleh ketersediaan berbagai ekstensi pendukung, seperti ESLint untuk menjaga kualitas dan konsistensi kode, Prettier untuk melakukan \textit{formatting} otomatis, serta Thunder Client yang dimanfaatkan untuk pengujian endpoint API secara lokal tanpa memerlukan aplikasi tambahan.

        \hspace*{1.25cm}Konfigurasi sistem dirancang dengan agar terpisah kode program. Informasi sensitif, seperti kredensial database, \textit{API key} layanan AI untuk Groq dan Google Generative AI, serta \textit{secret token} yang digunakan pada autentikasi JWT, disimpan dalam file environment \texttt{.env} yang tidak disertakan ke dalam \textit{version control system}. Pendekatan ini meningkatkan aspek keamanan sekaligus mempermudah pengelolaan konfigurasi pada berbagai lingkungan, seperti \textit{development}, \textit{staging}, dan \textit{production}, tanpa perlu melakukan perubahan pada kode sumber aplikasi. Seluruh konfigurasi tersebut kemudian dimuat ke dalam aplikasi NestJS menggunakan package \texttt{@nestjs/config}. Package ini memungkinkan akses variabel lingkungan secara terstruktur melalui mekanisme \textit{dependency injection}. Implementasi konfigurasi dilakukan dengan mendefinisikan \texttt{ConfigModule} sebagai \textit{global module}, sehingga setiap modul dalam aplikasi dapat mengakses parameter konfigurasi yang dibutuhkan secara aman dan konsisten, seperti yang ditunjukkan pada implementasi berikut:

        \begin{lstlisting}[language=Java, caption=Implementasi ConfigModule sebagai global configuration, label=lst:config-module]
import { Module } from '@nestjs/common';
import { ConfigModule } from '@nestjs/config';

@Module({
  imports: [
    ConfigModule.forRoot({
      envFilePath: ['.env'],
      isGlobal: true,
    }),
    // imports modul lainnya...
  ],
  controllers: [],
  providers: [],
})
export class AppModule {}
\end{lstlisting}

        \hspace*{1.25cm}Dengan konfigurasi tersebut, perubahan parameter sistem dapat dilakukan tanpa perlu memodifikasi kode sumber aplikasi. Penyesuaian cukup dilakukan dengan mengubah nilai pada file \textit{environment} sesuai dengan kebutuhan masing-masing lingkungan \textit{deployment}. Pendekatan ini tidak hanya menjaga kebersihan dan konsistensi kode, tetapi juga meminimalkan risiko kesalahan akibat perubahan langsung pada kode aplikasi.

  \item \textbf{Hasil Implementasi Basis Data}

        \hspace*{1.25cm}Basis data sistem berhasil diimplementasikan menggunakan PostgreSQL yang berperan sebagai penyimpan utama seluruh data aplikasi. PostgreSQL dipilih karena kemampuannya dalam menangani relasi data yang kompleks, serta dukungannya terhadap tipe data \textit{JSONB} yang sesuai untuk menyimpan hasil analisis sentimen dalam format semi-terstruktur. Di samping itu, PostgreSQL dikenal memiliki performa yang stabil, serta dukungan komunitas yang luas, sehingga memudahkan proses pengembangan dan pemeliharaan sistem dalam jangka panjang.

        \hspace*{1.25cm}Untuk mendukung kebutuhan \textit{Retrieval-Augmented Generation}, ekstensi \texttt{pgvector} berhasil diaktifkan agar database dapat menyimpan embedding teks dalam bentuk vektor numerik dan melakukan operasi \textit{similarity search} secara efisien. Aktivasi ekstensi ini merupakan langkah yang sederhana namun bersifat fundamental bagi keseluruhan sistem RAG, karena memungkinkan PostgreSQL tidak hanya berfungsi sebagai penyimpanan data relasional, tetapi juga sebagai \textit{vector store} yang mendukung pencarian berbasis kemiripan makna. Proses aktivasi dilakukan melalui perintah SQL khusus yang dijalankan pada database.

        \begin{lstlisting}[language=SQL, caption=Aktivasi ekstensi pgvector pada PostgreSQL, label=lst:pgvector-enable]
CREATE EXTENSION IF NOT EXISTS vector;
\end{lstlisting}

        \hspace*{1.25cm}Perintah aktivasi ekstensi tersebut berhasil menambahkan tipe data \texttt{vector(n)}, di mana $n$ merepresentasikan dimensi vektor, beserta berbagai operator dan fungsi yang mendukung proses \textit{similarity search}. Operator yang tersedia mencakup \textit{cosine distance} (\texttt{<=>}), \textit{L2 distance} (\texttt{<->}), dan \textit{inner product} (\texttt{<\#>}). Adanya ekstensi ini memungkinkan sistem melakukan pencarian berbasis kemiripan makna menggunakan metrik \textit{cosine similarity}, yang menjadi fondasi utama proses \textit{retrieval} dalam RAG. Dengan demikian, pencarian dokumen tidak lagi bergantung pada kecocokan kata kunci semata, melainkan pada kesamaan semantik antar teks.

        \hspace*{1.25cm}Pengelolaan skema database kemudian diimplementasikan menggunakan TypeORM sebagai \textit{Object-Relational Mapping (ORM)} sebagai tool untuk operasi database. Pemilihan TypeORM didasarkan pada kemampuannya menyederhanakan interaksi dengan database tanpa harus menulis query SQL secara manual, kemudahan dalam melakukan migrasi skema melalui mekanisme \textit{migration}, serta integrasinya yang berjalan secara mulus dengan ekosistem NestJS. Selain itu, TypeORM mendukung penerapan \textit{repository pattern} yang memisahkan logika akses data dari \textit{business logic}. Pendekatan ini membuat kode menjadi lebih terstruktur, mudah diuji, serta mempermudah proses pengembangan dan pemeliharaan sistem ke depannya. Struktur tabel, relasi antar entitas, serta \textit{constraints} berhasil didefinisikan secara konsisten sesuai dengan skema database yang telah dirancang pada tahap desain, mencakup tabel \texttt{users}, \texttt{scrape\_results}, \texttt{sentiment\_result}, \texttt{recommendation\_result}, serta \texttt{langchain\_documents} sebagai penyimpanan vektor.

  \item \textbf{Hasil Implementasi Arsitektur Backend}

        \hspace*{1.25cm}Arsitektur backend diimplementasikan dengan mengikuti prinsip \textit{modular architecture} yang menjadi karakteristik utama framework NestJS. Pendekatan ini memisahkan aplikasi ke dalam sejumlah modul yang bersifat independen, di mana setiap modul bertanggung jawab atas satu domain fungsional yang spesifik. Dengan struktur tersebut, modul-modul dapat dikembangkan, diuji, dan dipelihara secara terpisah tanpa saling bergantung secara langsung. Penerapan arsitektur modular memberikan beberapa keuntungan yang cukup signifikan. Kode aplikasi menjadi lebih terorganisir sehingga meningkatkan aspek \textit{maintainability}. Selain itu, proses \textit{scaling} menjadi lebih mudah ketika aplikasi berkembang, karena penambahan atau pengembangan fitur dapat difokuskan pada modul tertentu. Pendekatan ini juga mendukung kolaborasi tim pengembangan, karena setiap pengembang dapat bekerja pada modul yang berbeda tanpa menimbulkan konflik dengan pekerjaan pengembang lainnya.

        \begin{figure}[h!]
          \centering
          \begin{verbatim}
groq-chatbot/
|-- dist/
|-- node_modules/
|-- src/
|   |-- common/
|   |-- config/
|   |-- helpers/
|   |-- modules/
|   |   |-- absa/
|   |   |-- auth/
|   |   |   |-- decorators/
|   |   |   |-- dtos/
|   |   |   |   |-- login.dto.ts
|   |   |   |   -- refresh-token.dto.ts
|   |   |   |-- guards/
|   |   |   |-- interfaces/
|   |   |   |-- providers/
|   |   |   |   |-- auth.service.ts
|   |   |   |   -- generate-tokens.ts
|   |   |   |-- auth.controller.ts
|   |   |   -- auth.module.ts
|   |   |-- rag/
|   |   |-- scraping/
|   |   |-- umkm/
|   |   -- users/
|   |-- app.controller.ts
|   |-- app.module.ts
|   |-- app.service.ts
|   -- main.ts
|-- test/
-- .dockerignore
\end{verbatim}
          \caption{Struktur direktori proyek backend chatbot berbasis NestJS}
          \label{fig:struktur-folder-backend}
        \end{figure}

        \hspace*{1.25cm}Struktur folder pada Gambar \ref{fig:struktur-folder-backend} menunjukkan bahwa proyek backend disusun secara rapi dan modular berdasarkan fungsi masing-masing bagian. Pada level utama terdapat folder \texttt{dist/} yang berisi hasil kompilasi aplikasi, serta folder \texttt{node\_modules/} yang menyimpan seluruh pustaka pendukung yang dibutuhkan sistem.  Folder \texttt{src/} merupakan inti dari aplikasi karena berisi seluruh kode sumber. Di dalamnya terdapat beberapa folder pendukung, seperti \texttt{common/} untuk komponen yang digunakan secara umum di berbagai bagian aplikasi, \texttt{config/} untuk menyimpan pengaturan sistem, dan \texttt{helpers/} untuk fungsi-fungsi bantuan yang sering dipakai.

        \hspace*{1.25cm}Folder utama \texttt{modules/} mengelompokkan fitur aplikasi berdasarkan domainnya. Setiap subfolder di dalamnya mewakili satu fitur utama, seperti \texttt{auth/} untuk autentikasi pengguna, \texttt{absa/} untuk analisis sentimen berbasis aspek, \texttt{rag/} untuk chatbot berbasis RAG, \texttt{scraping/} untuk pengambilan data, \texttt{umkm/} untuk pengelolaan data UMKM, dan \texttt{users/} untuk manajemen pengguna. Setiap modul memiliki struktur yang seragam agar mudah dipahami dan dikembangkan. Sebagai contoh, modul \texttt{auth/} memiliki subfolder untuk mengatur validasi data, keamanan akses, serta logika bisnis terkait autentikasi. Selain itu, terdapat file controller untuk menangani permintaan dari pengguna dan file module untuk mengatur konfigurasi modul tersebut.

        \hspace*{1.25cm}Pada level \texttt{src/} juga terdapat file penting seperti \texttt{main.ts} sebagai titik awal jalannya aplikasi. Folder \texttt{test/} digunakan untuk menyimpan berkas pengujian guna memastikan setiap fitur berjalan dengan baik. Secara keseluruhan, struktur folder ini memudahkan pengembang dalam mengelola, memahami, dan mengembangkan sistem seiring bertambahnya fitur.


  \item \textbf{Hasil Implementasi Pipeline RAG}

        \hspace*{1.25cm}Pipeline \textit{Retrieval-Augmented Generation} berhasil diimplementasikan sebagai komponen inti dalam sistem chatbot, yang memungkinkan interpretasi hasil analisis sentimen melalui interaksi bahasa natural. Seluruh pipeline diorganisasikan di dalam \texttt{RAGModule}, yang mengintegrasikan berbagai proses utama, mulai dari pemuatan dokumen, pembuatan embedding, penyimpanan vektor, pencarian dokumen yang relevan, hingga generasi jawaban menggunakan \textit{Large Language Model}. Implementasi pipeline ini dibagi ke dalam dua fase utama, yaitu fase ingestion dan fase query, yang masing-masing memiliki peran berbeda namun saling melengkapi.

        \hspace*{1.25cm}RAGService berhasil diimplementasikan dengan mengadopsi interface \texttt{OnModuleInit} untuk menjalankan proses inisialisasi secara otomatis saat aplikasi pertama kali dijalankan. Pemanfaatan \textit{lifecycle method} ini memastikan bahwa seluruh komponen kritis dalam sistem RAG, seperti model LLM, model embedding, koneksi ke database, serta \textit{vector store}, telah terkonfigurasi dengan benar sebelum sistem mulai melayani permintaan dari pengguna. Dengan pendekatan ini, risiko kesalahan konfigurasi pada saat runtime dapat diminimalkan dan stabilitas sistem secara keseluruhan dapat terjaga.

        \begin{lstlisting}[language=Java, caption=Implementasi RAGService dengan automatic initialization, label=lst:rag-service-init]
@Injectable()
export class RAGService implements OnModuleInit {
  private readonly logger = new Logger(RAGService.name);
  private llm: ChatGroq;
  private embeddings: GoogleGenerativeAIEmbeddings;
  private vectorStore: PGVectorStore;
  private chain: RunnableSequence;
  private pool: Pool;

  constructor(
    private readonly configService: ConfigService,
    private readonly loadDocumentsProvider: LoadDocumentsProvider,
  ) {}

  async onModuleInit() {
    try {
      this.llm = new ChatGroq({
        apiKey: this.configService.get('llm.groq.apiKey'),
        model: this.configService.get('llm.groq.model'),
        temperature: 0.7,
        maxTokens: 8192,
      });

      this.embeddings = new GoogleGenerativeAIEmbeddings({
        apiKey: this.configService.get('llm.google.apiKey'),
        modelName: this.configService.get('llm.google.embeddingModel'),
      });

      this.pool = new Pool({
        host: this.configService.get('database.host'),
        port: this.configService.get('database.port'),
        user: this.configService.get('database.username'),
        password: this.configService.get('database.password'),
        database: this.configService.get('database.database'),
      });

      await this.initializeRAG();
      this.logger.log('RAG system berhasil diinisialisasi');
    } catch (error) {
      this.logger.error('Inisialisasi RAG gagal:', error);
      throw error;
    }
  }
}
\end{lstlisting}

        \hspace*{1.25cm}Tahap inisialisasi sistem berhasil mengonfigurasi model \textit{Large Language Model (LLM)} menggunakan layanan Groq, yang dipilih karena menyediakan model yang cukup banyak dengan harga yang terjaukau. Parameter \texttt{temperature} diatur pada nilai 0.7 untuk mencapai keseimbangan antara konsistensi jawaban dan variasi bahasa. Dengan pengaturan ini, model mampu menghasilkan respons yang terdengar natural tanpa menyimpang dari fakta yang terdapat dalam dokumen konteks. Selain itu, parameter \texttt{maxTokens} diset sebesar 8192 guna memberikan ruang yang memadai bagi model dalam menghasilkan jawaban yang panjang dan detail, khususnya untuk pertanyaan yang bersifat kompleks. Sistem juga berhasil menginisialisasi model embedding menggunakan \textit{Google Generative AI Embeddings} yang berfungsi mengonversi teks menjadi representasi vektor numerik yang merepresentasikan makna semantik. Di sisi lain, koneksi ke database PostgreSQL berhasil dibangun dengan memanfaatkan mekanisme \textit{connection pool}, yaitu penyediaan sejumlah koneksi database yang siap digunakan. Mekanisme ini meningkatkan efisiensi dan performa sistem, terutama ketika harus menangani banyak permintaan secara bersamaan.

        \hspace*{1.25cm}Selanjutnya fase ingestion berhasil diimplementasikan sebagai tahap persiapan \textit{knowledge base}. Pada tahap ini, data hasil analisis sentimen yang tersimpan dalam file JSON dimuat dan diolah menjadi dokumen teks yang lebih terstruktur. Pengelompokan dokumen dilakukan berdasarkan kategori informasi yang relevan, seperti ringkasan sentimen keseluruhan, faktor-faktor positif, dan faktor-faktor negatif, sehingga setiap dokumen merepresentasikan satu unit pengetahuan yang jelas.

        \hspace*{1.25cm}Proses ingestion yang ditunjukkan pada kode \ref{lst:ingestion-impl} dirancang dengan menerapkan prinsip \textit{idempotent}. Prinsip ini memastikan bahwa proses ingestion dapat dijalankan berulang kali tanpa menimbulkan duplikasi data maupun perubahan hasil yang tidak diinginkan. Dengan pendekatan tersebut, sistem tetap konsisten dan efisien, khususnya ketika aplikasi dijalankan ulang atau ketika proses inisialisasi perlu dilakukan kembali tanpa adanya pembaruan data.

        \begin{lstlisting}[language=Java, caption=Implementasi RAG Ingestion Pipeline dengan idempotent pattern, label=lst:ingestion-impl]
private async initializeRAG() {
  const documents = await this.loadDocumentsProvider.loadDocuments();
  this.logger.log(`Berhasil memuat ${documents.length} dokumen dari JSON`);

  const schema = this.configService.get('DATABASE_SCHEMA') || 'public';
  const table = this.configService.get('DATABASE_TABLE_NAME') || 'langchain_documents';
  const qualifiedForStore = `${schema}.${table}`;

  let existingCount = 0;
  try {
    const result = await this.pool.query(
      `SELECT COUNT(1) AS count FROM "${schema}"."${table}"`,
    );
    existingCount = parseInt(result.rows?.[0]?.count || '0', 10);
  } catch (e) {
    this.logger.warn('Tabel belum ada, akan dibuat saat ingestion');
    existingCount = 0;
  }

  if (existingCount > 0) {
    this.vectorStore = new PGVectorStore(this.embeddings, {
      pool: this.pool,
      tableName: qualifiedForStore,
    });
    this.logger.log('Menggunakan vector store yang sudah ada');
  } else {
    this.vectorStore = await PGVectorStore.fromDocuments(
      documents,
      this.embeddings,
      { pool: this.pool, tableName: qualifiedForStore },
    );
    this.logger.log(`Berhasil menyimpan ${documents.length} dokumen`);
  }

  await this.createRAGChain();
}
\end{lstlisting}

        \hspace*{1.25cm}Penerapan pola \textit{idempotent} pada proses ingestion memiliki peran yang sangat krusial, mengingat pembuatan embedding merupakan operasi yang relatif mahal baik dari sisi waktu komputasi maupun biaya pemanggilan API. Sistem berhasil melakukan pengecekan terhadap jumlah data yang telah tersimpan di dalam tabel \textit{vector store}. Apabila data tersebut sudah tersedia, sistem tidak melakukan proses penulisan ulang, melainkan hanya membuat instance \texttt{PGVectorStore} yang terhubung langsung ke tabel yang ada. Pendekatan ini terbukti efektif dalam menghemat waktu inisialisasi aplikasi sekaligus menekan biaya operasional, khususnya ketika aplikasi mengalami \textit{restart} tanpa adanya perubahan data pada \textit{knowledge base}.

        \hspace*{1.25cm}Proses pemuatan dokumen dijalankan oleh komponen \texttt{LoadDocumentsProvider} yang menerapkan strategi \textit{semantic chunking}. Strategi ini memecah data berdasarkan unit makna yang utuh, seperti ringkasan sentimen keseluruhan, sentimen per kategori, sentimen per brand, \textit{engagement} faktor-faktor positif, dan faktor-faktor negatif. Berbeda dengan pendekatan chunking berbasis ukuran karakter atau jumlah token, \textit{semantic chunking} yang diterapkan mampu mempertahankan konteks informasi secara menyeluruh di dalam setiap dokumen. Hasilnya, relevansi dokumen yang diperoleh pada tahap \textit{retrieval} meningkat, yang pada akhirnya berdampak positif terhadap kualitas jawaban yang dihasilkan oleh sistem.


        \begin{lstlisting}[language=Java, caption=Implementasi semantic chunking pada LoadDocumentsProvider, label=lst:load-docs-impl]
@Injectable()
export class LoadDocumentsProvider {
  async loadDocuments(): Promise<Document[]> {
    const documents: Document[] = [];
    const files = fs.readdirSync('data').filter(f => f.endsWith('.json'));

    for (const file of files) {
      const jsonData = JSON.parse(fs.readFileSync(`data/${file}`, 'utf-8'));

      if (jsonData.sentimentOverall) {
        documents.push(new Document({
          pageContent: jsonData.sentimentOverall,
          metadata: { source: file, type: 'sentiment_overview' },
        }));
      }

      if (jsonData.faktorPositifTop10) {
        documents.push(new Document({
          pageContent: jsonData.faktorPositifTop10,
          metadata: { source: file, type: 'positive_factors' },
        }));
      }

      if (jsonData.faktorNegatifTop10) {
        documents.push(new Document({
          pageContent: jsonData.faktorNegatifTop10,
          metadata: { source: file, type: 'negative_factors' },
        }));
      }
    }

    return documents;
  }
}
\end{lstlisting}

        \hspace*{1.25cm}Setiap dokumen yang dihasilkan berhasil dilengkapi dengan metadata yang berperan penting dalam memperkaya konteks sekaligus mempermudah proses \textit{retrieval}. Metadata \texttt{source} digunakan untuk menyimpan nama file asal dokumen, sehingga sistem tetap memiliki jejak terhadap sumber data yang digunakan. Sementara itu, metadata \texttt{type} berfungsi mengelompokkan dokumen berdasarkan jenis informasinya, seperti \texttt{sentiment\_overview}, \texttt{positive\_factors}, dan \texttt{negative\_factors}. Pemisahan ini memungkinkan sistem memberikan jawaban yang lebih fokus dan relevan ketika pengguna menanyakan aspek tertentu dari analisis sentimen.

        \hspace*{1.25cm}Selanjutnya fase query berhasil diimplementasikan untuk menangani keseluruhan proses, mulai dari penerimaan pertanyaan hingga generasi jawaban. Implementasi fase ini memanfaatkan \textit{LangChain Expression Language (LCEL)} yang menyusun alur eksekusi sebagai rangkaian proses berurutan, di mana setiap tahapan didefinisikan sebagai komponen modular yang saling terhubung. Ketika pengguna mengajukan pertanyaan, sistem mengonversi pertanyaan tersebut menjadi embedding, kemudian melakukan \textit{similarity search} dengan parameter \texttt{k: 3} untuk mengambil tiga dokumen yang paling relevan berdasarkan \textit{cosine similarity}.


        \begin{lstlisting}[language=Java, caption=Implementasi RAG Query Chain menggunakan LCEL, label=lst:query-chain-impl]
private async createRAGChain() {
  const prompt = PromptTemplate.fromTemplate(`
Konteks:
{context}

Pertanyaan:
{question}

Jawaban:
  `);

  this.chain = RunnableSequence.from([
    {
      context: async (input) => {
        const docs = await this.vectorStore
          .asRetriever({ k: 3 })
          .invoke(input.question);
        return docs.map(d => d.pageContent).join('\n');
      },
      question: (input) => input.question,
    },
    prompt,
    this.llm,
    new StringOutputParser(),
  ]);
}
\end{lstlisting}

        \hspace*{1.25cm}Pemilihan jumlah tiga dokumen pada proses \textit{retrieval} dipilih berdasarkan jumlah dokumen / \textit{chunk} yang ada. tiga dokumen mampu memberikan keseimbangan yang cukup baik antara kualitas konteks dan efisiensi sistem. Jumlah ini menyediakan informasi yang memadai untuk menjawab pertanyaan pengguna, tanpa membebani \textit{Large Language Model} dengan konteks berlebihan yang berpotensi menurunkan kualitas jawaban atau meningkatkan latensi sistem.

        \hspace*{1.25cm}Dokumen-dokumen hasil retrieval kemudian digabungkan menjadi satu teks konteks yang utuh dan disusun bersama pertanyaan pengguna ke dalam sebuah prompt terstruktur. Struktur prompt yang diterapkan memiliki pemisahan yang jelas antara bagian konteks hasil retrieval dan bagian pertanyaan pengguna. Pemisahan ini membantu model memahami bahwa jawaban yang dihasilkan harus berlandaskan pada informasi yang terdapat di dalam konteks, sehingga risiko terjadinya \textit{hallucination} dapat diminimalkan.

        \hspace*{1.25cm}Integrasi seluruh komponen sistem menghasilkan alur data yang jelas dan terstruktur, baik pada fase ingest maupun fase query. Pada fase ingest, sistem berhasil membaca file JSON hasil analisis sentimen, kemudian mengonversinya menjadi embedding menggunakan model \textit{Google Generative AI}. Embedding tersebut selanjutnya disimpan ke dalam PostgreSQL yang telah dilengkapi dengan ekstensi \texttt{pgvector}, sehingga data berada dalam format yang optimal untuk kebutuhan \textit{similarity search}. Secara ringkas, alur ini dapat direpresentasikan sebagai berikut:


        \begin{equation}
          \text{File JSON} \rightarrow \text{Dokumen / Chunking} \rightarrow \text{Embedding} \rightarrow \text{PostgreSQL (pgvector)}
        \end{equation}

        Pada fase query ketika pengguna mengajukan pertanyaan, alur proses yang berhasil diimplementasikan menjadi:

        \begin{equation}
          \text{Pertanyaan} \rightarrow \text{Embedding} \rightarrow \text{Similarity Search} \rightarrow \text{Prompt} \rightarrow \text{LLM} \rightarrow \text{Jawaban}
        \end{equation}

        \hspace*{1.25cm}Arsitektur modular yang berhasil diterapkan memungkinkan setiap komponen dalam pipeline, mulai dari pembacaan dokumen, pembuatan embedding, penyimpanan vektor, pencarian konteks, hingga proses pembuatan jawaban, dikembangkan dan dioptimasi secara independen.

  \item \textbf{Hasil Dokumentasi API Menggunakan Swagger}

        \begin{figure}[H]
          \centering
          \includegraphics[width=0.7\textwidth]{backend.sentinela.my.id_api_docs.png}
          \caption{Antarmuka Dokumentasi \textit{Swagger} API}
          \label{fig:swagger-ui}
        \end{figure}

        \hspace*{1.25cm}Pada gambar \ref{fig:swagger-ui} menampilkan antarmuka dokumentasi API sistem chatbot analisis sentimen UMKM yang dibangun menggunakan \textit{Swagger}. seluruh layanan API ditata secara terstruktur sehingga mudah dibaca, dipahami, dan diakses oleh pengembang maupun pihak terkait. Dokumentasi API ini berfungsi sebagai panduan visual yang menjelaskan layanan apa saja yang tersedia dalam sistem serta bagaimana cara menggunakannya. Melalui tampilan ini, pengembang dapat melihat daftar fitur utama sistem, memahami alur kerja layanan, dan mengetahui jenis informasi yang dapat dikirim maupun diterima oleh aplikasi. Antarmuka yang interaktif juga membantu mempermudah proses eksplorasi dan pengujian tanpa memerlukan alat tambahan.

        \hspace*{1.25cm}Secara garis besar, layanan API dikelompokkan ke dalam beberapa bagian utama yang mencerminkan komponen sistem, seperti modul \textit{user}, autentikasi, manajemen data scraping, analisis sentimen, \textit{ABSA}  serta chatbot berbasis \textit{Retrieval-Augmented Generation}. Pengelompokan ini memberikan gambaran yang jelas mengenai struktur sistem dan memudahkan navigasi antar fitur. Setiap layanan dalam dokumentasi dilengkapi dengan penjelasan singkat mengenai fungsinya sehingga pengguna dokumentasi dapat memahami tujuan dan kegunaannya secara cepat. Layanan yang memerlukan autentikasi khusus juga ditandai secara visual agar lebih mudah dikenali.

        \hspace*{1.25cm}Secara keseluruhan, penggunaan \textit{Swagger} dalam sistem ini tidak hanya berperan sebagai dokumentasi teknis, tetapi juga sebagai sarana komunikasi antar tim pengembang. Dokumentasi ini membantu memastikan bahwa seluruh pihak memiliki pemahaman yang sama mengenai cara kerja sistem, sehingga mendukung kelancaran pengembangan, pengujian, dan pemeliharaan di masa mendatang.

  \item \textbf{Hasil Antarmuka Chatbot}

        \hspace*{1.25cm}Gambar \ref{fig:chatbot-interface} menampilkan hasil implementasi antarmuka chatbot yang digunakan pada sistem analisis sentimen UMKM. Antarmuka ini dirancang dengan pendekatan yang sederhana dan responsif agar mudah digunakan oleh pengguna non-teknis, khususnya pelaku UMKM. Tampilan chatbot menyerupai aplikasi percakapan pada umumnya, sehingga pengguna dapat langsung berinteraksi tanpa memerlukan pembelajaran khusus.


        \begin{figure}[H]
          \centering
          \includegraphics[width=0.6\textwidth]{chatbot-ss.png}
          \caption{Antarmuka Chatbot}
          \label{fig:chatbot-interface}
        \end{figure}

        \hspace*{1.25cm}Antarmuka chatbot dikembangkan menggunakan React.js sebagai \textit{frontend framework} dan Tailwind CSS untuk pengelolaan tampilan visual. Penggunaan React.js memungkinkan antarmuka bersifat dinamis dan responsif terhadap interaksi pengguna, sementara Tailwind CSS membantu mempercepat proses perancangan tampilan dengan gaya yang konsisten dan modern. Terlihat pula adanya fitur pendukung seperti pesan pembuka otomatis, daftar pertanyaan yang disarankan, indikator status chatbot, serta kolom input untuk mengajukan pertanyaan secara bebas. Secara keseluruhan, antarmuka ini dirancang untuk mendukung pengalaman pengguna yang nyaman dan intuitif, sekaligus menjadi media utama interaksi antara pengguna dan sistem chatbot berbasis RAG yang telah dikembangkan pada sisi backend.
\end{enumerate}
\subsection{Hasil Pengujian dan Integrasi (Testing and Integration)}
Tahap pengujian dan integrasi dilakukan untuk memvalidasi fungsionalitas sistem serta memastikan bahwa seluruh komponen dapat bekerja secara terintegrasi. Pengujian dilakukan menggunakan metode Black Box Testing yang berfokus pada verifikasi kesesuaian antara input dan output tanpa memperhatikan struktur internal kode. Postman digunakan sebagai tools utama untuk melakukan pengujian terhadap seluruh  \textit{endpoint REST API} yang telah dikembangkan.

Pengujian dilakukan terhadap dua aspek utama sistem, yaitu pengujian fungsionalitas \textit{endpoint REST API} dan pengujian relevansi jawaban chatbot berbasis RAG. Pengujian \textit{endpoint} mencakup seluruh modul fungsional sistem, mulai dari endpoint dasar aplikasi, modul autentikasi, manajemen pengguna, layanan RAG, manajemen data UMKM, hingga manajemen hasil scraping. Setiap endpoint diuji menggunakan berbagai skenario yang mencakup kondisi normal (valid input), kondisi error (invalid input).

\begin{enumerate}
  \item \textbf{Hasil Pengujian Endpoint REST API}

        \hspace*{1.25cm}Tabel \ref{tab:hasil-pengujian-endpoint} menampilkan hasil pengujian fungsionalitas endpoint REST API sistem. Pengujian dilakukan dengan mengirimkan request ke setiap endpoint menggunakan berbagai skenario input, kemudian memverifikasi apakah response yang diterima sesuai dengan ekspektasi yang telah ditetapkan pada tahap desain.

        \begin{longtable}{|c|p{3.5cm}|p{1.5cm}|p{3.5cm}|p{3.5cm}|c|}
          \caption{Hasil Pengujian Fungsional Endpoint REST API} \label{tab:hasil-pengujian-endpoint}                                                                                                        \\
          \hline
          \textbf{No} & \textbf{Endpoint}                                 & \textbf{Method} & \textbf{Test Scenario}                   & \textbf{Expected Result}                          & \textbf{Status} \\
          \hline
          \endfirsthead

          \hline
          \textbf{No} & \textbf{Endpoint}                                 & \textbf{Method} & \textbf{Test Scenario}                   & \textbf{Expected Result}                          & \textbf{Status} \\
          \hline
          \endhead

          \hline
          \endfoot

          \hline
          \endlastfoot

          1           & /auth/login                                       & POST            & Kredensial valid                         & Access \& refresh token dikembalikan (HTTP 200)   & Passed          \\
          \hline

          2           & /auth/login                                       & POST            & Username tidak terdaftar                 & Pesan gagal autentikasi (HTTP 400)                & Passed          \\
          \hline

          3           & /auth/login                                       & POST            & Kata sandi salah                         & Pesan gagal autentikasi (HTTP 400)                & Passed          \\
          \hline

          4           & /auth/login                                       & POST            & Input tidak lengkap/format salah         & Pesan validasi (HTTP 400)                         & Passed          \\
          \hline

          5           & /auth/refresh                                     & POST            & Refresh token valid                      & Access token baru terbit (HTTP 200)               & Passed          \\
          \hline

          6           & /auth/refresh                                     & POST            & Refresh token kedaluwarsa/invalid        & Pesan gagal autentikasi (HTTP 401)                & Passed          \\
          \hline

          7           & /auth/me                                          & GET             & Permintaan dengan bearer token valid     & Data pengguna aktif dikembalikan (HTTP 200)       & Passed          \\
          \hline

          8           & /auth/me                                          & GET             & Permintaan tanpa/dengan token invalid    & Akses ditolak (HTTP 401)                          & Passed          \\
          \hline

          9           & /auth/logout                                      & POST            & Logout dengan token valid                & Sesi berakhir, HTTP 200                           & Passed          \\
          \hline

          10          & /auth/logout                                      & POST            & Logout tanpa token                       & Akses ditolak (HTTP 401)                          & Passed          \\
          \hline

          11          & /users/register                                   & POST            & Data lengkap dan unik                    & Akun dibuat (HTTP 201)                            & Passed          \\
          \hline

          12          & /users/register                                   & POST            & Username sudah terdaftar                 & Pesan gagal (HTTP 400)                            & Passed          \\
          \hline

          13          & /users/register                                   & POST            & Data kurang lengkap                      & Pesan validasi (HTTP 400)                         & Passed          \\
          \hline

          14          & /umkm                                             & GET             & Permintaan Data Sentimen Umkm            & Menampilkan summary data sentimen umkm (HTTP 200) & Passed          \\
          \hline

          15          & /rag/query                                        & POST            & Pertanyaan valid                         & Jawaban RAG dikembalikan (HTTP 200)               & Passed          \\
          \hline

          16          & /rag/query                                        & POST            & Payload kosong/invalid                   & Pesan validasi (HTTP 400)                         & Passed          \\
          \hline

          17          & /rag/query/:scraperId                             & POST            & Pertanyaan valid dengan token sah        & Jawaban berbasis data pengguna (HTTP 200)         & Passed          \\
          \hline

          18          & /rag/query/:scraperId                             & POST            & Permintaan tanpa token                   & Akses ditolak (HTTP 401)                          & Passed          \\
          \hline

          19          & /rag/query/:scraperId                             & POST            & Payload kosong/invalid                   & Pesan validasi (HTTP 400)                         & Passed          \\
          \hline

          20          & /rag/insights                                     & GET             & Permintaan insight sentimen umkm         & Insight sentimen umkm dikembalikan (HTTP 200)     & Passed          \\
          \hline

          21          & /rag/insights/ \allowbreak :scraperId             & GET             & Token valid, scraper ada                 & Insight scraper dikembalikan (HTTP 200)           & Passed          \\
          \hline

          22          & /rag/insights/ \allowbreak :scraperId             & GET             & Token tidak dikirim                      & Akses ditolak (HTTP 401)                          & Passed          \\
          \hline

          23          & /rag/insights/ \allowbreak :scraperId             & GET             & Scraper tidak ditemukan                  & Pesan tidak ditemukan (HTTP 404)                  & Passed          \\
          \hline

          24          & /scraping/results                                 & POST            & Data valid + file JSON sah + token valid & Data scraping tersimpan (HTTP 201)                & Passed          \\
          \hline

          25          & /scraping/results                                 & POST            & Tidak sertakan token                     & Akses ditolak (HTTP 401)                          & Passed          \\
          \hline

          26          & /scraping/results                                 & POST            & File hilang/format salah/ukuran \> 5MB   & Pesan validasi file (HTTP 400)                    & Passed          \\
          \hline

          27          & /scraping/results                                 & GET             & Pengguna punya data                      & Daftar hasil scraping sendiri (HTTP 200)          & Passed          \\
          \hline

          28          & /scraping/results                                 & GET             & Tanpa token                              & Akses ditolak (HTTP 401)                          & Passed          \\
          \hline

          29          & /scraping/results/:id                             & DELETE          & Hapus data milik sendiri                 & Data terhapus (HTTP 200)                          & Passed          \\
          \hline

          30          & /scraping/results/:id                             & DELETE          & ID tidak ditemukan                       & Pesan tidak ditemukan (HTTP 404)                  & Passed          \\
          \hline

          31          & /scraping/results/:id                             & DELETE          & Tanpa token                              & Akses ditolak (HTTP 401)                          & Passed          \\
          \hline

          32          & /scraping/results/:id/ \allowbreak download/csv   & GET             & Token valid, ID sah                      & File CSV terunduh (HTTP 200)                      & Passed          \\
          \hline

          33          & /scraping/results/:id/ \allowbreak download/csv   & GET             & Tanpa token                              & Akses ditolak (HTTP 401)                          & Passed          \\
          \hline

          34          & /scraping/results/:id/ \allowbreak download/csv   & GET             & ID tidak ditemukan                       & Pesan tidak ditemukan (HTTP 404)                  & Passed          \\
          \hline

          35          & /scraping/results/:id/ \allowbreak download/excel & GET             & Token valid, ID sah                      & File XLSX terunduh (HTTP 200)                     & Passed          \\
          \hline

          36          & /scraping/results/:id/ \allowbreak download/excel & GET             & Tanpa token                              & Akses ditolak (HTTP 401)                          & Passed          \\
          \hline

          37          & /scraping/results/:id/ \allowbreak download/excel & GET             & ID tidak ditemukan                       & Pesan tidak ditemukan (HTTP 404)                  & Passed          \\
          \hline

          38          & /absa/:scraperId                                  & POST            & Token valid, scraper ada                 & Analisis ABSA berhasil dibuat (HTTP 201)          & Passed          \\
          \hline

          39          & /absa/:scraperId                                  & POST            & Tanpa token                              & Akses ditolak (HTTP 401)                          & Passed          \\
          \hline

          40          & /absa/:scraperId                                  & POST            & scraperId bukan UUID valid               & Pesan validasi parameter (HTTP 400)               & Passed          \\
          \hline

          41          & /absa/:scraperId                                  & GET             & Token valid, data tersedia               & Hasil ABSA dikembalikan (HTTP 200)                & Passed          \\
          \hline

          42          & /absa/:scraperId                                  & GET             & Scraper belum dianalisis                 & Pesan tidak ditemukan (HTTP 404)                  & Passed          \\
          \hline

          43          & /absa/:scraperId                                  & GET             & Tanpa token                              & Akses ditolak (HTTP 401)                          & Passed          \\
          \hline

          44          & /absa/:scraperId/ \allowbreak recommendation      & GET             & Token valid, rekomendasi tersedia        & Rekomendasi berbasis ABSA (HTTP 200)              & Passed          \\
          \hline

          45          & /absa/:scraperId/ \allowbreak recommendation      & GET             & Scraper tidak ditemukan                  & Pesan tidak ditemukan (HTTP 404)                  & Passed          \\
          \hline

          46          & /absa/:scraperId/ \allowbreak recommendation      & GET             & Tanpa token                              & Akses ditolak (HTTP 401)                          & Passed          \\
          \hline
        \end{longtable}

        \hspace*{1.25cm}Berdasarkan hasil pengujian yang ditampilkan pada Tabel \ref{tab:hasil-pengujian-endpoint}, seluruh 46 skenario pengujian endpoint \textit{REST API} menunjukkan status \textbf{Passed}, yang mengindikasikan bahwa sistem telah beroperasi sesuai dengan spesifikasi yang ditetapkan. Pengujian mencakup berbagai aspek fungsional sistem, termasuk mekanisme autentikasi dan otorisasi yang melibatkan 10 skenario, manajemen pengguna dengan 3 skenario pada modul \textit{user}, serta layanan RAG yang diuji melalui 9 skenario berbeda. Hasil pengujian menunjukkan bahwa sistem mampu menangani validasi input dengan tepat, mengembalikan kode status \textit{HTTP} yang sesuai untuk setiap kondisi, dan memberikan pesan error yang informatif ketika terjadi kesalahan.

        \hspace*{1.25cm}Pengujian modul autentikasi membuktikan bahwa sistem dapat membedakan antara kredensial yang valid dan tidak valid, mengelola siklus hidup token dengan baik, serta menerapkan mekanisme otorisasi yang ketat pada \textit{endpoint} yang memerlukan autentikasi. Modul manajemen data scraping, yang mencakup 14 skenario pengujian, berhasil memvalidasi operasi \textit{create} dan \textit{delete} beserta fitur ekspor data dalam format CSV dan Excel. Sementara itu, modul \textit{ABSA} yang diuji melalui 9 skenario menunjukkan kemampuan sistem dalam melakukan analisis sentimen berbasis aspek dan menghasilkan rekomendasi strategis berdasarkan hasil analisis tersebut. Keberhasilan seluruh skenario pengujian ini mengonfirmasi bahwa integrasi antar-modul berjalan dengan baik dan sistem siap untuk memasuki tahap pengujian relevansi jawaban chatbot RAG.

  \item \textbf{Hasil Pengujian Relevansi Jawaban Chatbot RAG}

        \hspace*{1.25cm}Selain pengujian fungsionalitas endpoint, penelitian ini juga melakukan pengujian terhadap kualitas dan relevansi jawaban yang dihasilkan oleh sistem chatbot berbasis RAG. Fokus pengujian tidak lagi pada apakah layanan berjalan dengan benar, melainkan pada sejauh mana sistem mampu memahami pertanyaan pengguna dan menyusun jawaban yang sesuai dengan konteks data yang tersedia.

        \hspace*{1.25cm}Pengujian ini dirancang untuk memverifikasi kemampuan sistem dalam memberikan jawaban yang relevan, informatif, dan berbasis pada \textit{knowledge base} yang dimiliki. Dengan kata lain, evaluasi diarahkan untuk melihat apakah mekanisme \textit{retrieval} dan \textit{generation} dapat bekerja secara selaras ketika menghadapi berbagai jenis pertanyaan. Tabel \ref{tab:hasil-pengujian-rag} menyajikan ringkasan hasil pengujian relevansi jawaban chatbot terhadap beragam pertanyaan yang mewakili \textit{use case} umum dalam analisis sentimen UMKM.


        \begin{longtable}{|c|p{6cm}|p{6cm}|c|}
          \caption{Hasil Pengujian Relevansi Jawaban Chatbot RAG} \label{tab:hasil-pengujian-rag}                                                                                                                                                                                                                                                                                                                                                                         \\
          \hline
          \textbf{No} & \textbf{Pertanyaan (Questions)}                                       & \textbf{Jawaban (Answers)}                                                                                                                                                                                                                                                                                                                            & \textbf{Relevan?} \\
          \hline
          \endfirsthead

          \hline
          \textbf{No} & \textbf{Pertanyaan (Questions)}                                       & \textbf{Jawaban (Answers)}                                                                                                                                                                                                                                                                                                                            & \textbf{Relevan?} \\
          \hline
          \endhead

          \hline
          \endfoot

          \hline
          \endlastfoot

          1           & Apa maksud dari distribusi sentimen keseluruhan?                      & Distribusi sentimen menunjukkan bahwa mayoritas komentar bersifat netral, dengan 13.774 komentar (91,9\%), diikuti sentimen positif sebanyak 1.103 komentar (7,4\%), dan sentimen negatif hanya 107 komentar (0,7\%). Hal ini mengindikasikan bahwa sebagian besar pelanggan memberikan ulasan yang objektif tanpa kecenderungan emosional yang kuat. & Ya                \\
          \hline

          2           & Kategori kuliner mana yang memiliki rasio sentimen positif tertinggi? & Berdasarkan analisis data, kategori dengan rasio sentimen positif tertinggi adalah Kuliner - Makanan Tradisional dengan rasio positif 15.6\%.                                                                                                                                                                                                         & Ya                \\
          \hline

          3           & Brand mana yang memiliki rasio sentimen positif tertinggi?            & Brand dengan rasio sentimen positif tertinggi adalah Gacoan dengan 15.6\%, diikuti oleh Mieganbatte dengan 12.6\%.                                                                                                                                                                                                                                    & Ya                \\
          \hline

          4           & Bagaimana perbandingan rata-rata likes berdasarkan kategori sentimen? & Rata-rata likes tertinggi terdapat pada sentimen negatif (251.9), diikuti positif (251.2), dan netral (247.6). Data ini menunjukkan bahwa konten dengan sentimen negatif cenderung mendapat sedikit lebih banyak engagement dalam bentuk likes, kemungkinan karena konten kontroversial lebih menarik perhatian.                                      & Ya                \\
          \hline

          5           & Apa faktor positif yang paling sering muncul dalam ulasan?            & Kata yang paling sering muncul dalam sentimen positif adalah "enak" dengan 277 kemunculan, diikuti "pas" (162) dan "mantap" (143). Kata-kata ini mengindikasikan bahwa kualitas rasa menjadi faktor utama yang membuat pelanggan memberikan ulasan positif terhadap produk kuliner UMKM.                                                              & Ya                \\
          \hline

          6           & Siapa nama presiden indonesia?                                        & Maaf, saya tidak memiliki informasi tersebut dalam konteks yang diberikan.                                                                                                                                                                                                                                                                            & Ya                \\
          \hline

          7           & Bagaimana cara membuat pisang goreng?                                 & Maaf, saya tidak memiliki informasi tentang cara membuat pisang goreng dalam konteks yang diberikan.                                                                                                                                                                                                                                                  & Ya                \\
          \hline
        \end{longtable}

        \hspace*{1.25cm}Hasil pengujian menunjukkan bahwa sistem chatbot berbasis RAG mampu memberikan jawaban yang relevan dan kontekstual untuk berbagai jenis pertanyaan yang diajukan. Sistem dapat melakukan proses pencarian dokumen yang sesuai dengan konteks pertanyaan, kemudian memanfaatkan informasi tersebut untuk menghasilkan jawaban yang informatif dan relatif mudah dipahami, khususnya oleh pengguna non-teknis. Hal tersebut mengindikasikan bahwa implementasi pipeline RAG telah berjalan sesuai dengan tujuan penelitian, yaitu menjembatani data analitik yang kompleks dengan kebutuhan informasi praktis pelaku UMKM melalui interaksi bahasa alami.


\end{enumerate}

\section{Pembahasan}

Penelitian ini berhasil mengimplementasikan sebuah sistem chatbot analisis sentimen UMKM berbasis web dengan mengintegrasikan pendekatan \textit{Retrieval-Augmented Generation (RAG)}. Sistem dikembangkan menggunakan NestJS sebagai \textit{backend framework} dan LangChain.js sebagai orkestrator pipeline RAG. Seluruh proses implementasi diawali dari tahap analisis kebutuhan yang mengidentifikasi permasalahan utama, yaitu kesulitan pelaku UMKM dalam memahami hasil analisis sentimen yang umumnya disajikan dalam bentuk grafik dan tabel. Visualisasi semacam ini sering kali kurang ramah bagi pengguna non-teknis, khususnya pelaku UMKM yang tidak memiliki latar belakang di bidang statistik atau \textit{data science}. Temuan tersebut menjadi dasar perancangan solusi berbasis chatbot yang berperan sebagai jembatan antara data analitik yang kompleks dan kebutuhan informasi praktis pengguna.

Tahap spesifikasi sistem menghasilkan sebuah \textit{blueprint} kebutuhan yang terstruktur dan cukup detail, dengan empat komponen fungsional utama sebagai fondasi aplikasi. Pertama, sistem autentikasi dan autorisasi berbasis \textit{JSON Web Token} dipilih karena sifatnya yang \textit{stateless} dan efisien, memungkinkan proses verifikasi identitas tanpa ketergantungan pada penyimpanan sesi di sisi server. Kedua, modul manajemen data hasil \textit{scraping} Instagram dirancang dengan fungsi \textit{create} dan \textit{delete} untuk memudahkan pengelolaan data mentah yang menjadi input utama analisis sentimen. Ketiga, \textit{API Gateway} berperan sebagai lapisan orkestrasi yang menghubungkan frontend dengan layanan \textit{ABSA} dan \textit{Recommendation Content}, sehingga setiap komponen dapat dikembangkan dan dipelihara secara independen. Komponen keempat sekaligus menjadi inti penelitian adalah chatbot berbasis RAG, yang memungkinkan interaksi bahasa natural untuk memperoleh interpretasi kontekstual atas hasil analisis sentimen dengan mengombinasikan proses \textit{retrieval} dokumen relevan dan \textit{generation} jawaban menggunakan \textit{Large Language Model}.

Pada tahap desain, sistem menggunakan arsitektur tiga lapis yang memisahkan tanggung jawab secara jelas antara \textit{presentation layer}, \textit{application layer}, dan \textit{data layer}. Lapisan presentasi dikembangkan menggunakan React.js untuk menampilkan visualisasi data sentimen serta antarmuka chatbot yang intuitif. Seluruh logika bisnis dan pemrosesan data ditangani oleh backend NestJS pada lapisan aplikasi. Sementara itu, lapisan data memanfaatkan PostgreSQL dengan ekstensi \textit{pgvector} sebagai tempat penyimpanan, menerapkan pendekatan hibrida yang menggabungkan model relasional ter-normalisasi untuk data struktural dengan penyimpanan vektor untuk mendukung kebutuhan RAG. Salah satu keluaran penting dari tahap desain adalah perancangan pipeline RAG yang terdiri dari dua fase saling terkait, yaitu RAG Ingestion Pipeline dan RAG Query Pipeline.

Tahap implementasi diawali dengan penyiapan lingkungan pengembangan menggunakan Node.js versi LTS dan framework NestJS. Pemilihan teknologi ini didasarkan pada stabilitas, dukungan jangka panjang, serta kemampuannya menyediakan struktur modular yang sesuai untuk aplikasi berskala menengah hingga besar. Konfigurasi sistem dirancang dengan memisahkan kode aplikasi dan parameter lingkungan, di mana informasi sensitif seperti kredensial database dan \textit{API key} disimpan dalam file \textit{environment} yang tidak disertakan dalam \textit{version control system}. Pendekatan ini meningkatkan keamanan sekaligus mempermudah pengelolaan konfigurasi pada berbagai lingkungan \textit{deployment}. Implementasi basis data PostgreSQL berhasil mengaktifkan ekstensi \textit{pgvector}, yang menambahkan tipe data \textit{vector} serta operator \textit{similarity search} seperti \textit{cosine similarity}. Pengelolaan skema database dilakukan menggunakan TypeORM sebagai \textit{Object-Relational Mapping} untuk memisahkan logika akses data dari \textit{business logic}.

Arsitektur backend diimplementasikan mengikuti prinsip \textit{modular architecture}. Aplikasi dibagi ke dalam beberapa modul independen dengan tanggung jawab yang spesifik, seperti autentikasi, manajemen data, dan RAG. Struktur folder yang terorganisir memudahkan proses pengembangan, pengujian, serta pemeliharaan. Setiap modul memiliki struktur internal yang konsisten, mencakup \textit{decorators}, \textit{DTOs}, \textit{guards}, \textit{interfaces}, dan \textit{providers}. Pendekatan ini memberikan keuntungan dari sisi \textit{maintainability}, kemudahan \textit{scaling}, serta mendukung kolaborasi tim pengembang.

Pipeline RAG diimplementasikan sebagai komponen inti sistem dalam dua fase utama. Fase ingestion menerapkan prinsip \textit{idempotent} untuk menghindari pembuatan embedding ulang yang bersifat mahal dari sisi komputasi dan biaya. Sistem melakukan pengecekan data yang telah tersimpan sebelum melakukan penulisan ulang. Proses pemuatan dokumen menggunakan strategi \textit{semantic chunking} berdasarkan unit makna yang utuh, seperti ringkasan sentimen dan faktor positif maupun negatif, serta dilengkapi metadata untuk keperluan \textit{filtering} dokumen. Fase query diimplementasikan menggunakan \textit{LangChain Expression Language (LCEL)} dengan alur eksekusi berurutan. Pertanyaan pengguna diubah menjadi embedding, dilakukan \textit{similarity search} dengan parameter $k = 3$, lalu digabungkan ke dalam prompt terstruktur bersama konteks hasil retrieval untuk meminimalkan risiko halusinasi.

Pengujian sistem dilakukan menggunakan metode \textit{Black Box Testing} dengan Postman sebagai alat bantu. Hasil pengujian menunjukkan seluruh 46 skenario pengujian endpoint berhasil dilewati. Selain itu, pengujian relevansi jawaban chatbot terhadap 7 pertanyaan evaluasi menunjukkan bahwa sistem mampu melakukan pencarian dokumen yang sesuai dan menghasilkan jawaban yang informatif serta mudah dipahami oleh pengguna non-teknis. Secara keseluruhan, penelitian ini membuktikan bahwa pendekatan RAG efektif untuk menjembatani kesenjangan pemahaman antara visualisasi data analitik dan kebutuhan informasi praktis pelaku UMKM. Arsitektur modular dan pipeline RAG yang diimplementasikan memberikan fleksibilitas, efisiensi, serta performa yang baik.
